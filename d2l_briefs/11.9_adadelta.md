# [11.9 Adadelta](https://d2l.ai/chapter_optimization/adadelta.html)

- **Adadelta** is designed to improve the two main aspects of the **Adagrad**. <br> 1. the continual decay of learning rates throughout training. <br> 2. the need for a manually selected global learning rate. ([Zeiler 2012](https://arxiv.org/abs/1212.5701))

### Adadelta algorithm


&emsp;&emsp;&emsp;&emsp; <img src='./images/slide_adadelta.png' width='750'/>



- Instead of relyig on a single global learning rate  that applies to all parameter dimensions, **Adadelta** uses **the rate of change in the parameters** itself to adapt the learning rate (c.p. to eq.11.8.1 of [§11.8 RMSProp](./d2l_briefs/11.8_rmsprop.md)).



- Adadelta uses **leaky averages** to keep a running estimate of the **state statistics**, with the hyperparameter **ρ** controling the relative contributions from the **history of past steps**, for each update iteration. 


- The state variable **s**<sub>t</sub> is used to store the weighted history of **the 2nd moment of gradients**. <br> And Δ**w**<sub>t</sub> is used to store weighted history of **the 2nd moment in the change of parameters**.


- The intuition behind the design of Adadelta is inspired from the Newton's method (see [§11.3](./d2l_briefs/11.3_gradient-descent.md)). <br>
  - In Newton's method, for each iteration step the amount of update Δ**w** ∝ **g**/**H**, which has a natural dimension in **w**. 
  - In Adadelta, the amount of update Δ**w** is thus designed to have a dimension in **w**. 


- [Implementation in Pytorch](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html) : ``torch.optim.Adadelta(params, lr=1.0, rho=0.9)``. 
  - Note that here pytorch allows you to further adjust learning rate η to scale the updating step in eq.11.9.2, **w**<sub>t</sub> = **w**<sub>t-1</sub> - η**g**'<sub>t</sub>. The default learning rate parameter is set to be 1.

### Reference

- ADADELTA: AN ADAPTIVE LEARNING RATE METHOD -- Zeiler 2012 ([arxiv:1212.5701](https://arxiv.org/abs/1212.5701))
